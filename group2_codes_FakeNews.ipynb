{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "group2_codes_FakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RDgFfUk2O2jZ",
        "WP055--eO2jb",
        "EsR1xX7fO2k_",
        "C6kQqudjO2mt",
        "IKNxnUwjR5ec",
        "uFo998hVR5ej",
        "xJFsM-7QR5et",
        "k1fhkrvVO2qR",
        "zJU3J0I6O2qv",
        "Dj0ggLDrV3dZ",
        "hnDxC5JyO2rD",
        "fd1ws7n_O2rV",
        "IB9XmmysO2ri",
        "Z62HoxfMXPyq",
        "cCsTNqldZjSM",
        "Iigd8nHguRcK",
        "-mZeLt9tufXX",
        "HTx9bSB0vHa0",
        "TZQGaaz_vpKf",
        "OR5kQvUbxDTV",
        "Xqgj6mewO2rn"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CJRP0w_O2jG"
      },
      "source": [
        "# Course Project: Fake News Detection\n",
        "## Deep Neural Network Realization\n",
        "#### Group 2, Tinglin Duan, Tianyi Xie, Junxi Xu, Zhennan Ying, Qianyue Zhang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC16pvLrh3Nf"
      },
      "source": [
        "### 1. Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5qechv_O2jK"
      },
      "source": [
        "#### 1.1 Project Introduction\n",
        "This project is based on the Phase 1 of the $1 million Leaders Prize 2019: Fact or Fake News Challenge. This Challenge aims to stop misinformation and fake news spreading with Machine Learning based fact-checking. In Phase 1, each team is expected to design fact-checking algorithms to label claims as True, Partly True or False.\n",
        "\n",
        "Input: metadata file with claim information, training data is given with truth labels\n",
        "\n",
        "Output: predicted truth labels without any human intervention\n",
        "\n",
        "Note: in order to reproduce the result, one should have all the data files from the Datacup official website and download the stop_words.txt and embedding file from https://drive.google.com/drive/folders/1KbThhafJK5Aoa3zlvF-BbN2_QSDbRYXD?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G865SwaO2jM"
      },
      "source": [
        "#### 1.2 Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "E8UKq5HxO2jN",
        "outputId": "d379f51e-6201-4172-ece8-9278103cc286"
      },
      "source": [
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join, exists, split\n",
        "import collections\n",
        "import time\n",
        "import re\n",
        "import itertools\n",
        "import random\n",
        "import csv\n",
        "from string import punctuation\n",
        "import subprocess as sp\n",
        "from statistics import mode\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers.core import *\n",
        "from keras.layers import Multiply, Average, TimeDistributed\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.layers import Dense, Activation, Flatten, Input\n",
        "from keras.layers import Dropout, MaxPooling1D, Convolution1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Sequential, Model, model_from_json\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "from gensim.models import word2vec\n",
        "# print(device_lib.list_local_devices())\n",
        "np.random.seed(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "04JJ0nbUR5b8",
        "outputId": "a0058f12-e31e-4067-c967-a9dab1db2ab6"
      },
      "source": [
        "dir_path = os.path.dirname(os.path.realpath('__file__')) + '/'\n",
        "print(dir_path)\n",
        "Bookkeeping_path = dir_path + \"bookkeeping/\"\n",
        "if not os.path.exists(Bookkeeping_path):\n",
        "    os.makedirs(Bookkeeping_path)\n",
        "Model_path = dir_path + \"models/\"\n",
        "if not os.path.exists(Model_path):\n",
        "    os.makedirs(Model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf0b3OKgO2jU"
      },
      "source": [
        "#### 1.3 Global Static Variable Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWSueaaMO2jV"
      },
      "source": [
        "############################################### RNN PART ################################################\n",
        "stop_words_file = dir_path + 'stop_words.txt'\n",
        "article_folder = dir_path + 'train_articles'\n",
        "json_folder = dir_path + 'train.json'\n",
        "glove_data_file = dir_path + 'glove.6B.100d.txt'\n",
        "\n",
        "save_model_json_1 = Model_path + 'jupyter_trained_1.json'\n",
        "save_model_weights_1 = Model_path + 'jupyter_trained_1.h5'\n",
        "\n",
        "save_model_json_2 = Model_path + 'jupyter_trained_2.json'\n",
        "save_model_weights_2 = Model_path + 'jupyter_trained_2.h5'\n",
        "\n",
        "save_model_json_3 = Model_path + 'jupyter_trained_3.json'\n",
        "save_model_weights_3 = Model_path + 'jupyter_trained_3.h5'\n",
        "\n",
        "save_model_json_4 = Model_path + 'jupyter_trained_4.json'\n",
        "save_model_weights_4 = Model_path + 'jupyter_trained_4.h5'\n",
        "\n",
        "MAX_LEN_ART = 1500\n",
        "MAX_LEN_CLAIM = 100\n",
        "EMBEDDING_SIZE = 100\n",
        "unit_size = 256\n",
        "\n",
        "############################################### CNN PART ################################################\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 50\n",
        "filter_sizes = (3, 8)\n",
        "num_filters = 10\n",
        "dropout_prob = (0.5, 0.2)\n",
        "hidden_dims = 50\n",
        "output_dims = 3\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "num_epochs = 5 # early stopping\n",
        "\n",
        "# Word2Vec parameters (see train_word2vec)\n",
        "min_word_count = 1\n",
        "context = 10\n",
        "\n",
        "############################################### NB PART ################################################\n",
        "save_NBmodel = Model_path + 'NBmodel.sav'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDgFfUk2O2jZ"
      },
      "source": [
        "### 2 Training Data Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP055--eO2jb"
      },
      "source": [
        "#### 2.1 Data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDokKZjPO2jc"
      },
      "source": [
        "def word_split(paragraph):\n",
        "    return [splited.lower() for splited in nltk.word_tokenize(paragraph)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGnVPC67O2jf"
      },
      "source": [
        "def bar_plot(x, y, x_label, y_label, title):\n",
        "    y_pos = np.arange(len(x))\n",
        "    performance = y\n",
        "\n",
        "    plt.figure(figsize=(12,12))\n",
        "    plt.barh(y_pos, y, align='center', alpha=0.5)\n",
        "    plt.yticks(y_pos, x)\n",
        "    plt.ylabel(x_label)\n",
        "    plt.xlabel(y_label)\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SQOqhu3O2jj"
      },
      "source": [
        "with open(json_folder) as json_file:\n",
        "    df = (pd.DataFrame(json.load(json_file))).reset_index()\n",
        "    train, test = train_test_split(df, test_size=0.2)\n",
        "    train['claim_splitted'] = train.claim.apply(word_split)\n",
        "    test['claim_splitted'] = test.claim.apply(word_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GdaiE5wO2jm"
      },
      "source": [
        "The imported json file is divided into train and test sets. The test set in this case is just a validation set to see model jit level. The train/test proportion is set to be 0.8/0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0W8Xr4iO2jo",
        "outputId": "d4d8269c-3cfe-4e24-bb9f-964efcc21700"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>claim</th>\n",
              "      <th>claimant</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "      <th>related_articles</th>\n",
              "      <th>id</th>\n",
              "      <th>claim_splitted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3729</th>\n",
              "      <td>3729</td>\n",
              "      <td>Says U.S. Sen. Ron Johnson \"doesn't even belie...</td>\n",
              "      <td>Russ  Feingold</td>\n",
              "      <td>2016-10-06</td>\n",
              "      <td>1</td>\n",
              "      <td>[1088, 26723]</td>\n",
              "      <td>4107</td>\n",
              "      <td>[says, u.s., sen., ron, johnson, ``, does, n't...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12790</th>\n",
              "      <td>12790</td>\n",
              "      <td>“By denying climate change, [Stephen Harper] d...</td>\n",
              "      <td>Justin Trudeau</td>\n",
              "      <td>2015-07-09</td>\n",
              "      <td>0</td>\n",
              "      <td>[97541, 97901, 98022, 100860]</td>\n",
              "      <td>14091</td>\n",
              "      <td>[“, by, denying, climate, change, ,, [, stephe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3822</th>\n",
              "      <td>3822</td>\n",
              "      <td>Says President Donald Trump \"has signed more l...</td>\n",
              "      <td>Mike Pence</td>\n",
              "      <td>2017-07-18</td>\n",
              "      <td>1</td>\n",
              "      <td>[6007, 28240, 28245, 88386, 47172]</td>\n",
              "      <td>4210</td>\n",
              "      <td>[says, president, donald, trump, ``, has, sign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10346</th>\n",
              "      <td>10346</td>\n",
              "      <td>\"Obama's Private 'Security' Company Sets Up M...</td>\n",
              "      <td>Various websites</td>\n",
              "      <td>2018-04-30</td>\n",
              "      <td>0</td>\n",
              "      <td>[26939, 37849]</td>\n",
              "      <td>11405</td>\n",
              "      <td>[``, obama, 's, private, 'security, ', company...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4858</th>\n",
              "      <td>4858</td>\n",
              "      <td>“I was against the war in Iraq. Has not been d...</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>2016-10-09</td>\n",
              "      <td>0</td>\n",
              "      <td>[58855, 57620, 58877, 58858, 76663, 58736, 588...</td>\n",
              "      <td>5347</td>\n",
              "      <td>[“, i, was, against, the, war, in, iraq, ., ha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       index                                              claim  \\\n",
              "3729    3729  Says U.S. Sen. Ron Johnson \"doesn't even belie...   \n",
              "12790  12790  “By denying climate change, [Stephen Harper] d...   \n",
              "3822    3822  Says President Donald Trump \"has signed more l...   \n",
              "10346  10346   \"Obama's Private 'Security' Company Sets Up M...   \n",
              "4858    4858  “I was against the war in Iraq. Has not been d...   \n",
              "\n",
              "               claimant        date  label  \\\n",
              "3729     Russ  Feingold  2016-10-06      1   \n",
              "12790    Justin Trudeau  2015-07-09      0   \n",
              "3822         Mike Pence  2017-07-18      1   \n",
              "10346  Various websites  2018-04-30      0   \n",
              "4858       Donald Trump  2016-10-09      0   \n",
              "\n",
              "                                        related_articles     id  \\\n",
              "3729                                       [1088, 26723]   4107   \n",
              "12790                      [97541, 97901, 98022, 100860]  14091   \n",
              "3822                  [6007, 28240, 28245, 88386, 47172]   4210   \n",
              "10346                                     [26939, 37849]  11405   \n",
              "4858   [58855, 57620, 58877, 58858, 76663, 58736, 588...   5347   \n",
              "\n",
              "                                          claim_splitted  \n",
              "3729   [says, u.s., sen., ron, johnson, ``, does, n't...  \n",
              "12790  [“, by, denying, climate, change, ,, [, stephe...  \n",
              "3822   [says, president, donald, trump, ``, has, sign...  \n",
              "10346  [``, obama, 's, private, 'security, ', company...  \n",
              "4858   [“, i, was, against, the, war, in, iraq, ., ha...  "
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B3FGwwHO2ju",
        "outputId": "56fb6010-137e-42b4-a8ab-df636e85aef9"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>claim</th>\n",
              "      <th>claimant</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "      <th>related_articles</th>\n",
              "      <th>id</th>\n",
              "      <th>claim_splitted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11713</th>\n",
              "      <td>11713</td>\n",
              "      <td>\"Collusion is not a crime.\"</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>1</td>\n",
              "      <td>[42998, 18667, 43017, 33290, 48700, 50317, 50335]</td>\n",
              "      <td>12899</td>\n",
              "      <td>[``, collusion, is, not, a, crime, ., '']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3407</th>\n",
              "      <td>3407</td>\n",
              "      <td>\"It took us four years to balance the budget. ...</td>\n",
              "      <td>Bill Clinton</td>\n",
              "      <td>2012-06-02</td>\n",
              "      <td>1</td>\n",
              "      <td>[73809, 93667, 3636]</td>\n",
              "      <td>3745</td>\n",
              "      <td>[``, it, took, us, four, years, to, balance, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10621</th>\n",
              "      <td>10621</td>\n",
              "      <td>\"Obama keeps talking about getting rid of all ...</td>\n",
              "      <td>Dick Cheney</td>\n",
              "      <td>2015-07-14</td>\n",
              "      <td>1</td>\n",
              "      <td>[88354, 5530, 62990, 2507, 90824]</td>\n",
              "      <td>11705</td>\n",
              "      <td>[``, obama, keeps, talking, about, getting, ri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5506</th>\n",
              "      <td>5506</td>\n",
              "      <td>President Trump praised KFC founder Colonel Sa...</td>\n",
              "      <td></td>\n",
              "      <td>2017-05-22</td>\n",
              "      <td>0</td>\n",
              "      <td>[125263, 125503, 23295]</td>\n",
              "      <td>6064</td>\n",
              "      <td>[president, trump, praised, kfc, founder, colo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4567</th>\n",
              "      <td>4567</td>\n",
              "      <td>In 2008, \"only 54 percent of Latinos in Texas ...</td>\n",
              "      <td>Battleground Texas</td>\n",
              "      <td>2013-02-26</td>\n",
              "      <td>1</td>\n",
              "      <td>[15506, 16343, 15367]</td>\n",
              "      <td>5024</td>\n",
              "      <td>[in, 2008, ,, ``, only, 54, percent, of, latin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       index                                              claim  \\\n",
              "11713  11713                        \"Collusion is not a crime.\"   \n",
              "3407    3407  \"It took us four years to balance the budget. ...   \n",
              "10621  10621  \"Obama keeps talking about getting rid of all ...   \n",
              "5506    5506  President Trump praised KFC founder Colonel Sa...   \n",
              "4567    4567  In 2008, \"only 54 percent of Latinos in Texas ...   \n",
              "\n",
              "                 claimant        date  label  \\\n",
              "11713        Donald Trump  2018-07-31      1   \n",
              "3407         Bill Clinton  2012-06-02      1   \n",
              "10621         Dick Cheney  2015-07-14      1   \n",
              "5506                       2017-05-22      0   \n",
              "4567   Battleground Texas  2013-02-26      1   \n",
              "\n",
              "                                        related_articles     id  \\\n",
              "11713  [42998, 18667, 43017, 33290, 48700, 50317, 50335]  12899   \n",
              "3407                                [73809, 93667, 3636]   3745   \n",
              "10621                  [88354, 5530, 62990, 2507, 90824]  11705   \n",
              "5506                             [125263, 125503, 23295]   6064   \n",
              "4567                               [15506, 16343, 15367]   5024   \n",
              "\n",
              "                                          claim_splitted  \n",
              "11713          [``, collusion, is, not, a, crime, ., '']  \n",
              "3407   [``, it, took, us, four, years, to, balance, t...  \n",
              "10621  [``, obama, keeps, talking, about, getting, ri...  \n",
              "5506   [president, trump, praised, kfc, founder, colo...  \n",
              "4567   [in, 2008, ,, ``, only, 54, percent, of, latin...  "
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHfktz7LO2jy"
      },
      "source": [
        "Here is the json dictionary training data format:\n",
        "\n",
        "Main attributes: claim; claimant; date; label (0: false, 1: partly true, 2: true); related articles; id\n",
        "\n",
        "Example: {'claim': \"A line from George Orwell's novel 1984 predicts the power of smartphones.\", 'claimant': '', 'date': '2017-07-17', 'label': 0, 'related_articles': [122094, 122580, 130685, 134765], 'id': 0}\n",
        "\n",
        "In this project, it is desired to use the supported data to classify the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxs-hbRPO2jz",
        "outputId": "933cae59-4c1f-4d71-b972-49b4076740a2"
      },
      "source": [
        "claim_counter = {}\n",
        "\n",
        "for claim in df['claim']:\n",
        "    if len(claim) not in claim_counter:\n",
        "        claim_counter[len(claim)] = 1\n",
        "    else:\n",
        "        claim_counter[len(claim)] += 1\n",
        "\n",
        "print(Counter(claim_counter).most_common(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(86, 185), (88, 185), (76, 175), (87, 169), (92, 161), (78, 158), (85, 157), (73, 155), (77, 154), (101, 153)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-I-amLWO2j3",
        "outputId": "a0faf6ae-e819-4307-9ec3-4764ed4a3e7f"
      },
      "source": [
        "related_counter = {}\n",
        "\n",
        "for articles in df['related_articles']:\n",
        "    if len(articles) not in related_counter:\n",
        "        related_counter[len(articles)] = 1\n",
        "    else:\n",
        "        related_counter[len(articles)] += 1\n",
        "\n",
        "bar_plot(list(related_counter.keys()), list(related_counter.values()), 'related articles count', 'Appearances', 'related_articles counts')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsR1xX7fO2k_"
      },
      "source": [
        "#### 2.2 Data NAN filling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4LvURkgO2lB"
      },
      "source": [
        "def heat_plot(df):\n",
        "    fig, ax = plt.subplots(figsize=(15, 10))\n",
        "    sns.heatmap(df, cmap='coolwarm', yticklabels=False, cbar=False, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOhoNk7hO2lE",
        "outputId": "8b3f6399-2245-4ed0-d168-750ab3400a9c"
      },
      "source": [
        "heat_plot(df.isnull())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGwe4MuPO2lQ"
      },
      "source": [
        "This is a very appropriately filled dataset where no NAN entry has been found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prce1Vy7O2lR"
      },
      "source": [
        "#### 2.3 Project Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeWZypuoO2lS"
      },
      "source": [
        "In this part of the project, deep neural networks such as RNN, LSTM, CNN and Transformer are desired to be implemented. We are interested in the word sequence importance and text information rather than individual text features. Hence we will implement the model based on the columns \"claim\" and \"related_articles\" where we could evaluation text information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSUE4SJYO2lT"
      },
      "source": [
        "##### 2.3.1 LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXt7FT9wO2lU"
      },
      "source": [
        "Long Short Term Memory networks (LSTMs) are a special kinds of Recurrent Neural Networks (RNN), and was proposed by Hochreiter and Schmidhube as early as 1997. Compared with ordinary neural networks, the network is capable of remembering long term memories, which has proven to be essentially useful in speech recognition and sentimental analysis tasks. Compared with other existing RNN networks, it has three gates: input, output and forget. Considering the nature of this project involves text recognition, LSTM is a natural option. In this project's implementation, two LSTM layers with unit size 256 has been used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwl7KdU3O2lX"
      },
      "source": [
        "##### 2.3.2 Bi-directional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_FPkDk_O2lZ"
      },
      "source": [
        "Bidirectional LSTM is basically a LSTM with both directions, in other words, the states in the neural network has access to both \"past\" and \"future\" information. This kind of model with same parameters as the normal LSTM has been implemented as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1Rgsd_aO2ld"
      },
      "source": [
        "##### 2.3.3 LSTM with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW4BzC0TO2lg"
      },
      "source": [
        "During the years of implementation and discussion, gradient explosion and vanishing might happen with very long sentences, since we are multiplying and processing the gradients too many times. In order to resolve this matter, attention modules have been designed. In traditional LSTMs or RNNs, the network has been divided into \"encoding\" and \"decoding\" process, where the decoding process will only deal with the last context vector (or \"thought\" vector) at the end of the encoding process. This would cause the mentioned gradient explosion or vanishing since it would be very likely that the information of previous text would be lost along the way to generate the thought vector. Attention modules are designed such that all the intermediate states would be taken into consideration and have an impact on the decoding process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxwwGGv4O2lj"
      },
      "source": [
        "##### 2.3.4 LSTM with attention, supplied with extra feature related articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-miVFMIyO2ll"
      },
      "source": [
        "Apart from pure claim text analysis, it would also be valuable to take the related articles into accont. Checking reference sources is also a reliable way to tell if a news is valid for humans. Hence a deep neural network that takes the related articles text into consideration has been implemented as well. We averages the prediction distribution result with that of the previous claim LSTM and derives the final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gaiyw4dIbjSB"
      },
      "source": [
        "##### 2.3.5 Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQOikFhvcCfd"
      },
      "source": [
        "The Naive Bayes classifier is a probabilistic classifier. We basically compute the probability of each claim being in the each class (0: false, 1: partly true, 2: true). Since we are interested in the word sequence importance, we implement a model called Bag of Words to vectorize the claim based on the pre-constructed vocabulary. Then a Naive Bayes classifier is implemented to find the best class for each claim based on the maximum likelihood. However, Naive Bayes is a generative model which models the joint distribution of feature X and target Y to predict the P(y|x). It assumes all features to be conditionally independent, which means these features dont have correlation with each other and each feature is corresponding to one target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "299KTDJr5lTU"
      },
      "source": [
        "##### 2.3.6 Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvqq7mAv5rUr"
      },
      "source": [
        "CNN uses convolution in place of general matrix multiplication. Although CNN is not widely used in natural language processing, it is a powerful when handling complex classification problem. Based on the information provided in Yoon Kim's paper (https://arxiv.org/pdf/1408.5882v2.pdf), our group decided to implement a CNN classifier in order to compare the results woth the RNN classifiers mentioned above. As Kim's paper suggested, we also implemented an embedding layer taking advantage of the Word2vec function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMhn7tLhO2lm"
      },
      "source": [
        "### 3 Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLK1MeqVO2ly"
      },
      "source": [
        "def save_obj(obj, name):\n",
        "    with open(Bookkeeping_path + name + '.pkl', 'wb+') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(Bookkeeping_path + name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbW9AE3XO2l5"
      },
      "source": [
        "First we will perform basic preceesing to the splitted claims, excluding the punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpMZbOQGO2l7"
      },
      "source": [
        "# training part\n",
        "claim_list = train['claim'].values\n",
        "claimant_list = train['claimant'].values\n",
        "date_list = train['date'].values\n",
        "labels_list = list(train['label'].values)\n",
        "related_list = train['related_articles'].values\n",
        "id_list = list(train['id'].values)\n",
        "\n",
        "# testing part\n",
        "# we are interested in 'claim', 'related article'\n",
        "test_claim_list = test['claim'].values\n",
        "test_related_list = test['related_articles'].values\n",
        "test_labels_list = list(test['label'].values)\n",
        "\n",
        "# retrive all articles from the folder\n",
        "all_articles = [f for f in listdir(article_folder) if isfile(join(article_folder, f))]\n",
        "all_claims_splitted = []\n",
        "most_common_dic = {}\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    # preprocessing here, replace useless symbols\n",
        "    all_claims_splitted.append(' '.join(cp for cp in row['claim_splitted'] if cp not in punctuation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpLcRzRXO2mJ"
      },
      "source": [
        "Initilize a tokenizer and fit on all the claims"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A-s_xNrO2me"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_claims_splitted)\n",
        "\n",
        "save_obj(tokenizer, \"tokenizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6kQqudjO2mt"
      },
      "source": [
        "### 4 Feature Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whAEpiEaO2mv"
      },
      "source": [
        "In this part we will generate different kinds of features for the training part. We will do it two ways:\n",
        "\n",
        "1. [claim] purely claim as feature\n",
        "2. [claim, related_article text] claim + related article text as two features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqps_eHsO2my"
      },
      "source": [
        "def split_features(features, label):\n",
        "    features_split = []\n",
        "    label_list = []\n",
        "    for i in range(len(features[1])):\n",
        "        features_split.append([features[0], features[1][i]])\n",
        "        label_list.append(label)\n",
        "\n",
        "    return features_split, label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCHgpTI_O2m2"
      },
      "source": [
        "def feature_pre_generation(claim_list, related_list):\n",
        "    # vector [claim, related articles]\n",
        "    features = []\n",
        "    for i in range(claim_list.shape[0]):\n",
        "        features.append([claim_list[i], related_list[i]])\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MopddGrSO2m_"
      },
      "source": [
        "def feature_generation_claim_and_related(feature_list, label_list, X, Y):\n",
        "    for i in range(len(label_list)):\n",
        "        feat, lab = feature_list[i], label_list[i]\n",
        "        try:\n",
        "            split_feats, corresponding_labs = split_features(feat, lab)\n",
        "            for j in range(len(split_feats)):\n",
        "                X.append(split_feats[j])\n",
        "                Y.append(corresponding_labs[j])\n",
        "        except:\n",
        "            pass\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8MWfSK5O2nI"
      },
      "source": [
        "def fill_sets_one(X, Y, tokenizer, feature_list, label_list):\n",
        "    labels = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
        "    labels = np.array(labels)\n",
        "    for i in range(len(feature_list)):\n",
        "        claim_data = tokenizer.texts_to_sequences([feature_list[i][0]])\n",
        "        claim_data = pad_sequences(claim_data, maxlen=MAX_LEN_CLAIM, padding='post', truncating='post')[0]\n",
        "        X.append(claim_data)\n",
        "        Y.append(labels[label_list[i]])\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FyFEpPUO2nN"
      },
      "source": [
        "def fill_sets_two(X_c, X_a, Y, X_feature, Y_feature, tokenizer):\n",
        "    labels = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
        "    labels = np.array(labels)\n",
        "    for i in range(len(X_feature)):\n",
        "        claim_data = tokenizer.texts_to_sequences([X_feature[i][0]])\n",
        "        claim_data = pad_sequences(claim_data, maxlen=MAX_LEN_CLAIM, padding='post', truncating='post')[0]\n",
        "        X_c.append(claim_data)\n",
        "\n",
        "        related = X_feature[i][1]\n",
        "        with open(article_folder + '/' + str(related) + '.txt', 'r', encoding='utf-8') as f:\n",
        "            related_data = f.read()\n",
        "        related_data = tokenizer.texts_to_sequences([related_data])\n",
        "        art_data = pad_sequences(related_data, maxlen=MAX_LEN_ART, padding='post', truncating='post')[0]\n",
        "        X_a.append(art_data)\n",
        "        Y.append(labels[Y_feature[i]])\n",
        "        if (i % 5000 == 0):\n",
        "            print(\"training set {} of {} samples completed\".format(i, len(X_feature)))\n",
        "    return X_c, X_a, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBYMxB7EO2nQ"
      },
      "source": [
        "One hot encoding for labels: [1, 0, 0] is false, [0, 1, 0] is partly true, [0, 0, 1] is true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYPf9kDpO2nR"
      },
      "source": [
        "# generate training features\n",
        "feature_train = feature_pre_generation(claim_list, related_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgOTU5jtO2nV"
      },
      "source": [
        "# generate testing features\n",
        "feature_test = feature_pre_generation(test_claim_list, test_related_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQaFqzGVO2nZ"
      },
      "source": [
        "initialize training and test inputs, outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPTcRwiCO2nb"
      },
      "source": [
        "X_train_one = []\n",
        "Y_train_one = []\n",
        "X_test_one = []\n",
        "Y_test_one = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3W3zWSSO2ns"
      },
      "source": [
        "X_train_feature = []\n",
        "Y_train_feature = []\n",
        "X_test_feature = []\n",
        "Y_test_feature = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1F8ryH1O2n2"
      },
      "source": [
        "X_train_two_claim = []\n",
        "X_test_two_claim = []\n",
        "X_train_two_art = []\n",
        "Y_train_two = []\n",
        "X_test_two_art = []\n",
        "Y_test_two = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxpRlYhBO2n8"
      },
      "source": [
        "X_train_one, Y_train_one = fill_sets_one(X_train_one, Y_train_one, tokenizer, feature_train, labels_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODZO_dbiO2oE"
      },
      "source": [
        "X_test_one, Y_test_one = fill_sets_one(X_test_one, Y_test_one, tokenizer, feature_test, test_labels_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCnWV4BWO2oI"
      },
      "source": [
        "X_train_feature, Y_train_feature = feature_generation_claim_and_related(feature_train, labels_list, X_train_feature, Y_train_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0N_xFStO2oN"
      },
      "source": [
        "X_test_feature, Y_test_feature = feature_generation_claim_and_related(feature_test, test_labels_list, X_test_feature, Y_test_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-kJpixeO2oS",
        "outputId": "a47d9000-272a-4d0f-d41d-0a7849c4ff47"
      },
      "source": [
        "X_train_two_claim, X_train_two_art, Y_train_two = fill_sets_two(X_train_two_claim, X_train_two_art, Y_train_two, X_train_feature, Y_train_feature, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set 0 of 62053 samples completed\n",
            "training set 5000 of 62053 samples completed\n",
            "training set 10000 of 62053 samples completed\n",
            "training set 15000 of 62053 samples completed\n",
            "training set 20000 of 62053 samples completed\n",
            "training set 25000 of 62053 samples completed\n",
            "training set 30000 of 62053 samples completed\n",
            "training set 35000 of 62053 samples completed\n",
            "training set 40000 of 62053 samples completed\n",
            "training set 45000 of 62053 samples completed\n",
            "training set 50000 of 62053 samples completed\n",
            "training set 55000 of 62053 samples completed\n",
            "training set 60000 of 62053 samples completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHLQwvDCO2oX",
        "outputId": "e9ad6b97-6260-4d12-a993-f28d17620697"
      },
      "source": [
        "X_test_two_claim, X_test_two_art, Y_test_two = fill_sets_two(X_test_two_claim, X_test_two_art, Y_test_two, X_test_feature, Y_test_feature, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set 0 of 15627 samples completed\n",
            "training set 5000 of 15627 samples completed\n",
            "training set 10000 of 15627 samples completed\n",
            "training set 15000 of 15627 samples completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej9hcwWmO2oa"
      },
      "source": [
        "X_train_one = np.array(X_train_one)\n",
        "Y_train_one = np.array(Y_train_one)\n",
        "X_test_one = np.array(X_test_one)\n",
        "Y_test_one = np.array(Y_test_one)\n",
        "\n",
        "X_train_two_claim = np.array(X_train_two_claim)\n",
        "X_test_two_claim = np.array(X_test_two_claim)\n",
        "X_train_two_art = np.array(X_train_two_art)\n",
        "X_test_two_art = np.array(X_test_two_art)\n",
        "Y_train_two = np.array(Y_train_two)\n",
        "Y_test_two = np.array(Y_test_two)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL86joVmO2oe",
        "outputId": "ebb34783-a9d2-43aa-960e-9369534c4106"
      },
      "source": [
        "X_train_one.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12444, 100)"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS8jwIcmO2ok",
        "outputId": "013631f5-fbad-4267-eb6e-dcdb3b0d2a86"
      },
      "source": [
        "Y_train_one.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12444, 3)"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gAvQ6wFO2op",
        "outputId": "247ce980-797a-4502-935b-7fda19f6c1f0"
      },
      "source": [
        "X_test_one.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3111, 100)"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beBafxm4O2oy",
        "outputId": "0e1beb12-4950-4935-f5e5-05be81912814"
      },
      "source": [
        "Y_test_one.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3111, 3)"
            ]
          },
          "execution_count": 36,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvYZCGzgO2pe",
        "outputId": "fcac1a95-a646-4b80-8d61-8a49ea4ae5f9"
      },
      "source": [
        "X_train_two_claim.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(62053, 100)"
            ]
          },
          "execution_count": 37,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGXVefIPO2pp",
        "outputId": "c51dcc99-7645-4417-e266-f035507dfdba"
      },
      "source": [
        "X_test_two_claim.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15627, 100)"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfRadP4sO2pu",
        "outputId": "b9dfe537-d0c8-4549-c2e7-8d2d33c115a4"
      },
      "source": [
        "X_train_two_art.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(62053, 1500)"
            ]
          },
          "execution_count": 39,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_je4HmBVO2py",
        "outputId": "c25063b8-4ce8-4ecc-89de-022729e876b4"
      },
      "source": [
        "X_test_two_art.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15627, 1500)"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsMDOcr5O2qB",
        "outputId": "b7d739aa-55f9-43e7-9175-b3144cf67f43"
      },
      "source": [
        "Y_train_two.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(62053, 3)"
            ]
          },
          "execution_count": 41,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pa8GkkCO2qL",
        "outputId": "3d26eaf6-1314-46c2-a531-82517f8377db"
      },
      "source": [
        "Y_test_two.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15627, 3)"
            ]
          },
          "execution_count": 42,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKNxnUwjR5ec"
      },
      "source": [
        "### 5 Naive Bayes Implementation\n",
        "A NB approach has been implemented as a reference\n",
        "#### 5.1 Bag of Words Model\n",
        "Bag of Words Model is used to convert sentences (claims) into vectors. In Natrual Language Processing, this model can be used to extract features from text. The first step is to generate a vocabulary, which is a list of distinct words extracted from our claims. Then each claim is vectorized based on the vocabulary. The output for this model is a numerical vector representing each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdXfiKi_taVa"
      },
      "source": [
        "claims = []\n",
        "for claim in train['claim']:\n",
        "  claims.append(claim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYcdzNILtaVc"
      },
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzKJ72U_R5ec"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = stop_words, max_features = 10000) \n",
        "train_data_features = vectorizer.fit_transform(claims)\n",
        "bagofwords_data = []\n",
        "for claim in train['claim']:\n",
        "  bagofwords_list = vectorizer.transform([claim]).toarray()\n",
        "  bagofwords_data.append(bagofwords_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jZb2brbR5ef"
      },
      "source": [
        "X = pd.DataFrame(bagofwords_data, dtype=np.int8)\n",
        "y = list(train['label'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJFsM-7QR5et"
      },
      "source": [
        "#### 5.2 Multinomial NB Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAp0xl08R5ew",
        "outputId": "1b060ede-feb6-4f6d-831a-bb04f2f8896f"
      },
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.3)\n",
        "NBmodel = MultinomialNB()\n",
        "NBmodel.fit(X_train, y_train)\n",
        "\n",
        "pickle.dump(NBmodel, open(save_NBmodel, 'wb'))\n",
        " \n",
        "# some time later...\n",
        " \n",
        "# load the model from disk\n",
        "NBmodel = pickle.load(open(save_NBmodel, 'rb'))\n",
        "\n",
        "y_train_predict = NBmodel.predict(X_train)\n",
        "y_validation_predict = NBmodel.predict(X_validation)\n",
        "train_accuracy = f1_score(y_train, y_train_predict, average='micro')\n",
        "validation_accuracy = f1_score(y_validation, y_validation_predict,average='micro')\n",
        "print(\"Train accuracy: {}\".format(train_accuracy))\n",
        "print(\"Validation accuracy: {}\".format(validation_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.7468427095292767\n",
            "Validation accuracy: 0.5715050883770755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMX1cNf7yCB7"
      },
      "source": [
        "#### 5.3 Apply the Fitted NB Model on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIeZS6FRyCB8"
      },
      "source": [
        "bagofwords_data_test = []\n",
        "for claim in test['claim']:\n",
        "  bagofwords_list_test = vectorizer.transform([claim]).toarray()\n",
        "  bagofwords_data_test.append(bagofwords_list_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlxKc8kQyCB8"
      },
      "source": [
        "X_test = pd.DataFrame(bagofwords_data_test, dtype=np.int8)\n",
        "y_test = list(test['label'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1pNwS-dyCB9",
        "outputId": "6d78adae-aabe-4312-bd0c-559de8b13df6"
      },
      "source": [
        "y_test_predict = NBmodel.predict(X_test)\n",
        "test_accuracy = f1_score(y_test, y_test_predict,average='micro')\n",
        "print(\"Test accuracy: {}\".format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.5789135326261652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1fhkrvVO2qR"
      },
      "source": [
        "### 6 Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55mDaX5BO2qS"
      },
      "source": [
        "With text features generated and fit tokenizer in hand, we could perform word embedding and generate word embedding (weight) matrix. In this project we would use the pretrained embedding from glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj44KShWO2qT"
      },
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t6iuPFNO2qX"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(glove_data_file, encoding='utf-8', errors='ignore')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FXY1J-JO2ql"
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_SIZE))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJU3J0I6O2qv"
      },
      "source": [
        "### 7 Model Implementations (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj0ggLDrV3dZ"
      },
      "source": [
        "#### 7.1 General LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egKjWrHoO2qw",
        "outputId": "d0e19902-0748-4086-e377-ea22e93bd15f"
      },
      "source": [
        "model_one = Sequential()\n",
        "embedding_layer = Embedding(input_dim=len(word_index)+1, output_dim=EMBEDDING_SIZE,  weights=[embedding_matrix], input_length=MAX_LEN_CLAIM, trainable=False)\n",
        "\n",
        "model_one.add(embedding_layer)\n",
        "model_one.add(LSTM(unit_size, return_sequences=True))\n",
        "model_one.add(LSTM(unit_size, return_sequences=True))\n",
        "model_one.add(Flatten())\n",
        "model_one.add(Dense(3, activation='softmax'))\n",
        "model_one.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model_one.fit(X_train_one, Y_train_one, epochs=5, batch_size=64)\n",
        "print(model_one.summary())\n",
        "\n",
        "# save model\n",
        "model_json = model_one.to_json()\n",
        "with open(save_model_json_1, \"w+\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model_one.save_weights(save_model_weights_1)\n",
        "print(\"model saved\")\n",
        "\n",
        "# load model\n",
        "json_file = open(save_model_json_1, 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights(save_model_weights_1)\n",
        "print(\"model loaded\")\n",
        "\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "scores = loaded_model.evaluate(X_test_one, Y_test_one, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Louise\\Anaconda3\\envs\\CMU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Louise\\Anaconda3\\envs\\CMU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Louise\\Anaconda3\\envs\\CMU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Louise\\Anaconda3\\envs\\CMU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Louise\\Anaconda3\\envs\\CMU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Louise\\Anaconda3\\envs\\CMU\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\Louise\\Anaconda3\\envs\\CMU\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/5\n",
            "12444/12444 [==============================] - 48s 4ms/step - loss: 0.9344 - acc: 0.5544\n",
            "Epoch 2/5\n",
            "12444/12444 [==============================] - 47s 4ms/step - loss: 0.8871 - acc: 0.5823\n",
            "Epoch 3/5\n",
            "12444/12444 [==============================] - 47s 4ms/step - loss: 0.8633 - acc: 0.6035\n",
            "Epoch 4/5\n",
            "12444/12444 [==============================] - 47s 4ms/step - loss: 0.8386 - acc: 0.6138\n",
            "Epoch 5/5\n",
            "12444/12444 [==============================] - 47s 4ms/step - loss: 0.8032 - acc: 0.6345\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          1966000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 256)          365568    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100, 256)          525312    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25600)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 76803     \n",
            "=================================================================\n",
            "Total params: 2,933,683\n",
            "Trainable params: 967,683\n",
            "Non-trainable params: 1,966,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "model saved\n",
            "model loaded\n",
            "Accuracy: 58.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnDxC5JyO2rD"
      },
      "source": [
        "#### 7.2 Bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmjh8nrUO2rG",
        "outputId": "d0e4c930-6538-4785-853d-73c1e1e5c9f3"
      },
      "source": [
        "model_one_bi = Sequential()\n",
        "embedding_layer = Embedding(input_dim=len(word_index)+1, output_dim=EMBEDDING_SIZE,  weights=[embedding_matrix], input_length=MAX_LEN_CLAIM, trainable=False)\n",
        "\n",
        "model_one_bi.add(embedding_layer)\n",
        "model_one_bi.add(Bidirectional(LSTM(unit_size, return_sequences=True, dropout=0.25, recurrent_dropout=0.1)))\n",
        "model_one_bi.add(Bidirectional(LSTM(unit_size, return_sequences=True, dropout=0.25, recurrent_dropout=0.1)))\n",
        "model_one_bi.add(Flatten())\n",
        "model_one_bi.add(Dense(3, activation='softmax'))\n",
        "model_one_bi.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model_one_bi.fit(X_train_one, Y_train_one, epochs=5, batch_size=64)\n",
        "print(model_one_bi.summary())\n",
        "\n",
        "# save model\n",
        "model_json = model_one_bi.to_json()\n",
        "with open(save_model_json_2, \"w+\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model_one_bi.save_weights(save_model_weights_2)\n",
        "print(\"model saved\")\n",
        "\n",
        "# load model\n",
        "json_file = open(save_model_json_2, 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights(save_model_weights_2)\n",
        "print(\"model loaded\")             \n",
        "       \n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "scores = loaded_model.evaluate(X_test_one, Y_test_one, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Louise\\Anaconda3\\envs\\CMU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Epoch 1/5\n",
            "12444/12444 [==============================] - 93s 7ms/step - loss: 0.9897 - acc: 0.5215\n",
            "Epoch 2/5\n",
            "12444/12444 [==============================] - 91s 7ms/step - loss: 0.9144 - acc: 0.5611\n",
            "Epoch 3/5\n",
            "12444/12444 [==============================] - 92s 7ms/step - loss: 0.8894 - acc: 0.5877\n",
            "Epoch 4/5\n",
            "12444/12444 [==============================] - 91s 7ms/step - loss: 0.8713 - acc: 0.5974\n",
            "Epoch 5/5\n",
            "12444/12444 [==============================] - 91s 7ms/step - loss: 0.8428 - acc: 0.6155\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 100)          1966000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 512)          731136    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 100, 512)          1574912   \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 51200)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 153603    \n",
            "=================================================================\n",
            "Total params: 4,425,651\n",
            "Trainable params: 2,459,651\n",
            "Non-trainable params: 1,966,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "model saved\n",
            "model loaded\n",
            "Accuracy: 55.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd1ws7n_O2rV"
      },
      "source": [
        "#### 7.3 LSTM with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIehDEoZO2rc",
        "outputId": "816f1486-729d-45ae-e765-2f1658096dc7"
      },
      "source": [
        "_input = Input(shape=[MAX_LEN_CLAIM], dtype='int32')\n",
        "embedded = Embedding(input_dim=len(word_index)+1, output_dim=EMBEDDING_SIZE,  weights=[embedding_matrix], input_length=MAX_LEN_CLAIM, trainable=False)(_input)\n",
        "model_LSTM = LSTM(unit_size, return_sequences=True)(embedded)\n",
        "model_LSTM = LSTM(unit_size, return_sequences=True)(model_LSTM)\n",
        "\n",
        "model_attention = TimeDistributed(Dense(1, activation='tanh'))(model_LSTM)\n",
        "model_attention = Flatten()(model_attention)\n",
        "model_attention = Activation('softmax')(model_attention)\n",
        "model_attention = RepeatVector(unit_size)(model_attention)\n",
        "model_attention = Permute([2, 1])(model_attention)\n",
        "\n",
        "merged = Multiply()([model_LSTM, model_attention])\n",
        "merged = Lambda(lambda xin: K.sum(xin, axis=1))(merged)\n",
        "distributions = Dense(3, activation='softmax')(merged)\n",
        "\n",
        "model_one_attention = Model(inputs=_input, outputs=distributions)\n",
        "model_one_attention.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model_one_attention.fit(X_train_one, Y_train_one, epochs=5, batch_size=64)\n",
        "print(model_one_attention.summary())\n",
        "                           \n",
        "# save model\n",
        "model_json = model_one_attention.to_json()\n",
        "with open(save_model_json_3, \"w+\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model_one_attention.save_weights(save_model_weights_3)\n",
        "print(\"model saved\")\n",
        "\n",
        "# load model\n",
        "json_file = open(save_model_json_3, 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights(save_model_weights_3)\n",
        "print(\"model loaded\")                                \n",
        "          \n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "scores = loaded_model.evaluate(X_test_one, Y_test_one, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "12444/12444 [==============================] - 48s 4ms/step - loss: 0.9407 - acc: 0.5494\n",
            "Epoch 2/5\n",
            "12444/12444 [==============================] - 47s 4ms/step - loss: 0.8957 - acc: 0.5803\n",
            "Epoch 3/5\n",
            "12444/12444 [==============================] - 47s 4ms/step - loss: 0.8820 - acc: 0.5903\n",
            "Epoch 4/5\n",
            "12444/12444 [==============================] - 47s 4ms/step - loss: 0.8690 - acc: 0.5974\n",
            "Epoch 5/5\n",
            "12444/12444 [==============================] - 47s 4ms/step - loss: 0.8561 - acc: 0.6062\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 100, 100)     1966000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 100, 256)     365568      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 100, 256)     525312      lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 100, 1)       257         lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 100)          0           time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 100)          0           flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 256, 100)     0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 100, 256)     0           repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 100, 256)     0           lstm_6[0][0]                     \n",
            "                                                                 permute_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 256)          0           multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            771         lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,857,908\n",
            "Trainable params: 891,908\n",
            "Non-trainable params: 1,966,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "model saved\n",
            "model loaded\n",
            "Accuracy: 59.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB9XmmysO2ri"
      },
      "source": [
        "#### 7.4 LSTM with attention, supplied with extra feature related articles\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjKe1H61O2rk",
        "outputId": "9ee39f0a-8145-490d-9a79-e2ac3c13a9c0"
      },
      "source": [
        "_input = Input(shape=[MAX_LEN_CLAIM], dtype='int32')\n",
        "embedded = Embedding(input_dim=len(word_index)+1, output_dim=EMBEDDING_SIZE,  weights=[embedding_matrix], input_length=MAX_LEN_CLAIM, trainable=False)(_input)\n",
        "model_LSTM = LSTM(unit_size, return_sequences=True)(embedded)\n",
        "model_LSTM = LSTM(unit_size, return_sequences=True)(model_LSTM)\n",
        "\n",
        "model_attention = TimeDistributed(Dense(1, activation='tanh'))(model_LSTM)\n",
        "model_attention = Flatten()(model_attention)\n",
        "model_attention = Activation('softmax')(model_attention)\n",
        "model_attention = RepeatVector(unit_size)(model_attention)\n",
        "model_attention = Permute([2, 1])(model_attention)\n",
        "\n",
        "merged = Multiply()([model_LSTM, model_attention])\n",
        "merged = Lambda(lambda xin: K.sum(xin, axis=1))(merged)\n",
        "distributions = Dense(3, activation='softmax')(merged)\n",
        "\n",
        "_input_art = Input(shape=[MAX_LEN_ART], dtype='int32')\n",
        "embedded_art = Embedding(input_dim=len(word_index)+1, output_dim=EMBEDDING_SIZE,  weights=[embedding_matrix], input_length=MAX_LEN_ART, trainable=False)(_input_art)\n",
        "model_LSTM_art = LSTM(unit_size, return_sequences=True)(embedded_art)\n",
        "model_LSTM_art = LSTM(unit_size, return_sequences=True)(model_LSTM_art)\n",
        "\n",
        "model_attention_art = TimeDistributed(Dense(1, activation='tanh'))(model_LSTM_art)\n",
        "model_attention_art = Flatten()(model_attention_art)\n",
        "model_attention_art = Activation('softmax')(model_attention_art)\n",
        "model_attention_art = RepeatVector(unit_size)(model_attention_art)\n",
        "model_attention_art = Permute([2, 1])(model_attention_art)\n",
        "\n",
        "merged_art = Multiply()([model_LSTM_art, model_attention_art])\n",
        "merged_art = Lambda(lambda xin: K.sum(xin, axis=1))(merged_art)\n",
        "distributions_art = Dense(3, activation='softmax')(merged_art)\n",
        "\n",
        "merged_distribution = Average()([distributions, distributions_art])\n",
        "\n",
        "model = Model(inputs=[_input, _input_art], outputs=merged_distribution)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model.fit([X_train_two_claim, X_train_two_art], Y_train_two, epochs=3, batch_size=64)\n",
        "print(model.summary())\n",
        "\n",
        "# save model\n",
        "model_json = model.to_json()\n",
        "with open(save_model_json_4, \"w+\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(save_model_weights_4)\n",
        "print(\"model saved\")\n",
        "\n",
        "# load model\n",
        "json_file = open(save_model_json_4, 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "loaded_model.load_weights(save_model_weights_4)\n",
        "print(\"model loaded\")\n",
        "\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "scores = loaded_model.evaluate([X_test_two_claim, X_test_two_art], Y_test_two, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "62053/62053 [==============================] - 3590s 58ms/step - loss: 0.8631 - acc: 0.5966\n",
            "Epoch 2/3\n",
            "62053/62053 [==============================] - 3580s 58ms/step - loss: 0.7560 - acc: 0.6766\n",
            "Epoch 3/3\n",
            "62053/62053 [==============================] - 3582s 58ms/step - loss: 0.6348 - acc: 0.7662\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 1500)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 100, 100)     1966000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 1500, 100)    1966000     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 100, 256)     365568      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 1500, 256)    365568      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   (None, 100, 256)     525312      lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, 1500, 256)    525312      lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 100, 1)       257         lstm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 1500, 1)      257         lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 100)          0           time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1500)         0           time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 100)          0           flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1500)         0           flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_2 (RepeatVector)  (None, 256, 100)     0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_3 (RepeatVector)  (None, 256, 1500)    0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_2 (Permute)             (None, 100, 256)     0           repeat_vector_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "permute_3 (Permute)             (None, 1500, 256)    0           repeat_vector_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 100, 256)     0           lstm_8[0][0]                     \n",
            "                                                                 permute_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 1500, 256)    0           lstm_10[0][0]                    \n",
            "                                                                 permute_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 256)          0           multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 256)          0           multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 3)            771         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 3)            771         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "average_1 (Average)             (None, 3)            0           dense_6[0][0]                    \n",
            "                                                                 dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,715,816\n",
            "Trainable params: 1,783,816\n",
            "Non-trainable params: 3,932,000\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "model saved\n",
            "model loaded\n",
            "Accuracy: 57.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw1V4ZtGCab2"
      },
      "source": [
        "It has been noticed that the bidirectional LSTM structure does not improve model accuracy. However, the attention model noticeably enhances the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iigd8nHguRcK"
      },
      "source": [
        "### 8 Model Implementations (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mZeLt9tufXX"
      },
      "source": [
        "#### 8.1 Feature Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a-Eb02yuj_j"
      },
      "source": [
        "# split training data and labels\n",
        "def load_data_and_labels():\n",
        "    # Load data from json\n",
        "    json_folder = './train.json'\n",
        "    with open(json_folder) as json_file:\n",
        "        df = (pd.DataFrame(json.load(json_file))).reset_index()\n",
        "        df['claim_splitted'] = df.claim.apply(word_split)\n",
        "\n",
        "    # define training data and labels\n",
        "    claim_list = list(df['claim_splitted'].values)\n",
        "    claimant_list = df['claimant'].values\n",
        "    related_list = df['related_articles'].values\n",
        "    id_list = list(df['id'].values)\n",
        "    labels_list = list(df['label'].values)\n",
        "\n",
        "    x_text = claim_list\n",
        "    labels = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "    y = []\n",
        "    for i in range(len(labels_list)) :\n",
        "        y.append(labels[labels_list[i]])\n",
        "    return x_text, y\n",
        "\n",
        "# build the vocaulary mapping for the embedded layer\n",
        "def build_vocab(sentences):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary mapping from word to index based on the sentences.\n",
        "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
        "    \"\"\"\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return vocabulary, vocabulary_inv\n",
        "\n",
        "# set the final training data and labels\n",
        "def build_input_data(sentences, labels, vocabulary):\n",
        "    \"\"\"\n",
        "    Maps sentencs and labels to vectors based on a vocabulary.\n",
        "    \"\"\"\n",
        "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
        "    y = np.array(labels)\n",
        "    return x, y\n",
        "\n",
        "# sentence padding\n",
        "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
        "    \"\"\"\n",
        "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
        "    Returns padded sentences.\n",
        "    \"\"\"\n",
        "    sequence_length = max(len(x) for x in sentences)\n",
        "    padded_sentences = []\n",
        "    for i in range(len(sentences)):\n",
        "        sentence = sentences[i]\n",
        "        num_padding = sequence_length - len(sentence)\n",
        "        new_sentence = sentence + [padding_word] * num_padding\n",
        "        padded_sentences.append(new_sentence)\n",
        "    return padded_sentences\n",
        "\n",
        "# load data\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads and preprocessed data for the MR dataset.\n",
        "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
        "    \"\"\"\n",
        "    # Load and preprocess data\n",
        "    sentences, labels = load_data_and_labels()\n",
        "    sentences_padded = pad_sentences(sentences)\n",
        "    vocabulary, vocabulary_inv_list = build_vocab(sentences_padded)\n",
        "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
        "    vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
        "\n",
        "    # Shuffle data\n",
        "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "    x = x[shuffle_indices]\n",
        "    y = y[shuffle_indices]\n",
        "    train_len = int(len(x) * 0.9)\n",
        "    x_train = x[:train_len]\n",
        "    y_train = y[:train_len]\n",
        "    x_test = x[train_len:]\n",
        "    y_test = y[train_len:]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test, vocabulary_inv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "kPl3qncFvEv3",
        "outputId": "5090d6b5-2787-4e9c-892e-404c5daba9ef"
      },
      "source": [
        "print(\"Load data...\")\n",
        "x_train, y_train, x_test, y_test, vocabulary_inv = load_data()\n",
        "\n",
        "# truncate the over-padded training samples due to the matrix size\n",
        "# shrinkage after word2vec\n",
        "x_train = x_train[:,:MAX_LEN_CLAIM]\n",
        "x_test = x_test[:,:MAX_LEN_CLAIM]\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load data...\n",
            "x_train shape: (13999, 100)\n",
            "x_test shape: (1556, 100)\n",
            "Vocabulary Size: 24096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTx9bSB0vHa0"
      },
      "source": [
        "#### 8.2 Training the embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIHCR0o-vNrN"
      },
      "source": [
        "I used a model online to train words into vectors. The weights will be used as the initialization weights of the embedding vector.\n",
        "\n",
        "This function could be found at https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/w2v.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM0p7fo9veuC"
      },
      "source": [
        "def train_word2vec(sentence_matrix, vocabulary_inv,\n",
        "                   num_features=300, min_word_count=1, context=10):\n",
        "    \"\"\"\n",
        "    Trains, saves, loads Word2Vec model\n",
        "    Returns initial weights for embedding layer.\n",
        "\n",
        "    inputs:\n",
        "    sentence_matrix # int matrix: num_sentences x max_sentence_len\n",
        "    vocabulary_inv  # dict {int: str}\n",
        "    num_features    # Word vector dimensionality\n",
        "    min_word_count  # Minimum word count\n",
        "    context         # Context window size\n",
        "    \"\"\"\n",
        "    model_dir = '.'\n",
        "    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
        "    model_name = join(model_dir, model_name)\n",
        "    if exists(model_name):\n",
        "        embedding_model = word2vec.Word2Vec.load(model_name)\n",
        "        print('Load existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
        "    else:\n",
        "        # Set values for various parameters\n",
        "        num_workers = 2  # Number of threads to run in parallel\n",
        "        downsampling = 1e-3  # Downsample setting for frequent words\n",
        "\n",
        "        # Initialize and train the model\n",
        "        print('Training Word2Vec model...')\n",
        "        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n",
        "        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
        "                  size=num_features, min_count=min_word_count,\n",
        "                  window=context, sample=downsampling)\n",
        "\n",
        "        # If we don't plan to train the model any further, calling\n",
        "        # init_sims will make the model much more memory-efficient.\n",
        "        embedding_model.init_sims(replace=True)\n",
        "\n",
        "        # Saving the model for later use. You can load it later using Word2Vec.load()\n",
        "        if not exists(model_dir):\n",
        "            os.mkdir(model_dir)\n",
        "        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
        "        embedding_model.save(model_name)\n",
        "\n",
        "    # add unknown words\n",
        "    embedding_weights = {key: embedding_model[word] if word in embedding_model else\n",
        "                              np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
        "                         for key, word in vocabulary_inv.items()}\n",
        "    return embedding_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZQGaaz_vpKf"
      },
      "source": [
        "#### 8.3 Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIc_vv18vroc"
      },
      "source": [
        "# Build model\n",
        "input_shape = (MAX_LEN_CLAIM,)\n",
        "model_input = Input(shape=input_shape)\n",
        "\n",
        "z = Embedding(len(vocabulary_inv), embedding_dim, input_length=MAX_LEN_CLAIM, name=\"embedding\")(model_input)\n",
        "\n",
        "z = Dropout(dropout_prob[0])(z)\n",
        "\n",
        "# Convolutional block\n",
        "conv_blocks = []\n",
        "for sz in filter_sizes:\n",
        "    conv = Convolution1D(filters=num_filters,\n",
        "                         kernel_size=sz,\n",
        "                         padding=\"valid\",\n",
        "                         activation=\"relu\",\n",
        "                         strides=1)(z)\n",
        "    conv = MaxPooling1D(pool_size=2)(conv)\n",
        "    conv = Flatten()(conv)\n",
        "    conv_blocks.append(conv)\n",
        "\n",
        "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "\n",
        "z = Dropout(dropout_prob[1])(z)\n",
        "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
        "model_output = Dense(output_dims, activation=\"softmax\")(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR5kQvUbxDTV"
      },
      "source": [
        "#### 8.4 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "yiPg7PpoxFOf",
        "outputId": "1b7a7db6-54a8-489e-dee1-846a4f167f0a"
      },
      "source": [
        "# Prepare embedding layer weights\n",
        "\n",
        "embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
        "                    min_word_count=min_word_count, context=context)\n",
        "\n",
        "model = Model(model_input, model_output)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Save model for future use\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Initialize weights with word2vec\n",
        "weights = np.array([v for v in embedding_weights.values()])\n",
        "print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
        "embedding_layer = model.get_layer(\"embedding\")\n",
        "embedding_layer.set_weights([weights])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
        "          validation_data=(x_test, y_test), verbose=2)\n",
        "\n",
        "# Save model weights for future use\n",
        "model.save('cnn_acc.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load existing Word2Vec model '50features_1minwords_10context'\n",
            "Initializing embedding layer with word2vec weights, shape (24096, 50)\n",
            "Train on 13999 samples, validate on 1556 samples\n",
            "Epoch 1/5\n",
            " - 11s - loss: 0.9363 - acc: 0.5355 - val_loss: 0.9125 - val_acc: 0.6022\n",
            "Epoch 2/5\n",
            " - 10s - loss: 0.8855 - acc: 0.5980 - val_loss: 0.8897 - val_acc: 0.6157\n",
            "Epoch 3/5\n",
            " - 10s - loss: 0.8364 - acc: 0.6340 - val_loss: 0.8681 - val_acc: 0.6247\n",
            "Epoch 4/5\n",
            " - 10s - loss: 0.7703 - acc: 0.6733 - val_loss: 0.8535 - val_acc: 0.6228\n",
            "Epoch 5/5\n",
            " - 10s - loss: 0.6946 - acc: 0.7140 - val_loss: 0.8867 - val_acc: 0.6067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzlhYZ1DB6W4"
      },
      "source": [
        "We observed a validation accuracy of 60.67% after 5 epochs. We noticed that our CNN model would not classify any news as \"True news\". This might caused by the lack of \"True news\" samlples in the provided dataset, making related information hard to survive when passing the convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqgj6mewO2rn"
      },
      "source": [
        "### 9 Discussions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AtjbfpJO2rp"
      },
      "source": [
        "From the previous section, it has been noticed that the bidirectional LSTM structure does not improve model accuracy. However, the attention model noticeably enhances the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86_uoiZMB0OP"
      },
      "source": [
        "In sum, we have implemented in total three different models for this specific project: 1) Naive Bayes 2) RNN and 3) CNN, where we have used the Naive Bayes model with Bag of Words vectorization as a benchmark of our implementation. Our NB model has a training accuracy of almost 75%, however the testing accuracy is just over 57%, revealing large variance. In our RNN realization, we have compared between four different LSTM models and found that the LSTM with attention model performs best on the testing data set, having an accuracy of almost 60%. We have also adopted pre-trained glove word embedding matrices in our model construction. Finally, we investigated upon a novel CNN approach with word embeddings. The model acted fair during training and has an 60% validation accuracy. Nevertheless, the model does not have the ability to label True News thus its perfomance on a larger data set might be worse. Compared with out NB benchmark, both the RNN and CNN model behave better and after evaluation we have decided to use the RNN (with attention) model in our final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5eispiwCrWZ"
      },
      "source": [
        "Thanks for the great course and happy holidays!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paA6AK_gO2rv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}